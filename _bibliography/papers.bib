---
---

@string{aps = {American Physical Society,}}

@article{fang2026simhum,
  abbr={arXiv},
  bibtex_show={true},
  title={Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation},
  author={Fang, Kaipeng and Liang, Weiqing and Li, Yuyang and Zhang, Ji and Zeng, Pengpeng and Gao, Lianli and Song, Jingkuan and Shen, Heng Tao},
  journal={arXiv preprint arXiv:2601.19406},
  year={2026},
  selected={true},
  tldr={SimHum is a co-training framework that simultaneously extracts kinematic prior from simulated robot actions and visual prior from real-world human observations, achieving data-efficient and generalizable robotic manipulation with up to 40% improvement over baselines and 62.5% OOD success with only 80 real data.},
  arxiv={2601.19406},
  website={https://kaipengfang.github.io/sim-and-human},
  preview={simhum_2026.png}
}

@inproceedings{fang2024pros,
  abbr={CVPR},
  bibtex_show={true},
  title={ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval},
  author={Fang, Kaipeng and Song, Jingkuan and Gao, Lianli and Zeng, Pengpeng and Cheng, Zhi-Qi and Li, Xiyao and Shen, Heng Tao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  selected={true},
  tldr={This paper proposes Prompting-to-Simulate (ProS), a novel prompt tuning framework for Universal Cross-Domain Retrieval (UCDR) that generates Content-aware Dynamic Prompts via a two-stage simulation process to effectively address domain and semantic shifts, achieving state-of-the-art performance with high parameter efficiency.},
  arxiv={2312.12478},
  code={https://github.com/kaipengfang/ProS},
  preview={pros_2024.png},
  google_scholar_id={9yKSN-GCB0IC}
}

